import cloudscraper
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time

# –ê–¥—Ä–µ—Å–∏ –±–µ–∑ –∑–º—ñ–Ω
ADDRESSES = {
    "1.1": {"city": "–ß–æ—Ä—Ç–∫—ñ–≤", "street": "–≤—É–ª. –†–∏–Ω–æ–∫", "house": "1"},
    "1.2": {"city": "–¢–µ—Ä–Ω–æ–ø—ñ–ª—å", "street": "–≤—É–ª. –í–æ–ª–æ–¥–∏–º–∏—Ä–∞ –õ—É—á–∞–∫–æ–≤—Å—å–∫–æ–≥–æ", "house": "1"},
    "2.1": {"city": "–ö—Ä–µ–º–µ–Ω–µ—Ü—å", "street": "–≤—É–ª. –ê–Ω–¥—Ä—ñ—è –ü—É—à–∫–∞—Ä—è", "house": "3"},
    "2.2": {"city": "–Ü–≤–∞–Ω—á–∞–Ω–∏", "street": "–≤—É–ª. –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞", "house": "1"},
    "3.1": {"city": "–õ–∏—Å–∏—á–∏–Ω—Ü—ñ", "street": "–≤—É–ª. –õ–µ—Å—ñ –£–∫—Ä–∞—ó–Ω–∫–∏", "house": "1"},
    "3.2": {"city": "–¢–µ—Ä–Ω–æ–ø—ñ–ª—å", "street": "–≤—É–ª. –ú–∏—Ö–∞–π–ª–∞ –í–µ—Ä–±–∏—Ü—å–∫–æ–≥–æ", "house": "4"},
    "4.1": {"city": "–¢–µ—Ä–Ω–æ–ø—ñ–ª—å", "street": "–≤—É–ª. –ú–∏–∫–æ–ª–∏ –ü–∏—Ä–æ–≥–æ–≤–∞", "house": "1"},
    "4.2": {"city": "–†—É–±–ª–∏–Ω", "street": "–≤—É–ª. –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞", "house": "1"},
    "5.1": {"city": "–ô–æ—Å–∏–ø—ñ–≤–∫–∞", "street": "–≤—É–ª. –õ–µ—Å—ñ –£–∫—Ä–∞—ó–Ω–∫–∏", "house": "18"},
    "5.2": {"city": "–¢–µ—Ä–Ω–æ–ø—ñ–ª—å", "street": "–≤—É–ª. –î–º–∏—Ç—Ä–∞ –í–∏—à–Ω–µ–≤–µ—Ü—å–∫–æ–≥–æ", "house": "1"},
    "6.1": {"city": "–î–∞–Ω–∏–ª—ñ–≤—Ü—ñ", "street": "–≤—É–ª. –ú–æ–ª–æ–¥—ñ–∂–Ω–∞", "house": "292"},
    "6.2": {"city": "–ë–æ—Ä—â—ñ–≤", "street": "–≤—É–ª. –†–æ–º–∞–Ω–∞ –®—É—Ö–µ–≤–∏—á–∞", "house": "1"}
}

def get_schedule(scraper, group, addr):
    url = "https://www.toe.com.ua/index.php/pohodynni-vidkliuchennia"
    payload = {'city': addr['city'], 'street': addr['street'], 'house': addr['house'], 'action': 'search'}
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Origin': 'https://www.toe.com.ua',
        'Referer': 'https://www.toe.com.ua/index.php/pohodynni-vidkliuchennia'
    }
    
    try:
        response = scraper.post(url, data=payload, headers=headers, timeout=45)
        if "cloudflare" in response.text.lower() or response.status_code == 403:
            print(f"üõë {group}: –ë–ª–æ–∫—É–≤–∞–Ω–Ω—è Cloudflare")
            return [1] * 24

        soup = BeautifulSoup(response.text, 'html.parser')
        hours_data = []
        
        # –ü–æ—à—É–∫ –∫–æ–ª—å–æ—Ä—ñ–≤ –ø–ª–∏—Ç–æ–∫
        for el in soup.find_all(True, style=True):
            txt = el.get_text(strip=True)
            if len(txt) == 5 and txt[2] == ':':
                style = el.get('style', '').lower()
                if '0, 0, 51' in style or '#000033' in style: hours_data.append(0)
                elif 'gray' in style or 'gradient' in style or '80, 80, 80' in style: hours_data.append(2)
                else: hours_data.append(1)
        
        return hours_data[-24:] if len(hours_data) >= 24 else [1] * 24
    except Exception as e:
        print(f"‚ùå {group}: {e}")
        return [1] * 24

# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é
scraper = cloudscraper.create_scraper(delay=10)

print("‚è≥ –ó–∞—Ö–æ–¥–∏–º–æ –Ω–∞ —Å–∞–π—Ç...")
try:
    scraper.get("https://www.toe.com.ua/index.php/pohodynni-vidkliuchennia")
    time.sleep(35) # –ß–µ–∫–∞—î–º–æ, –ø–æ–∫–∏ Cloudflare "–ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç—å" –Ω–∞—Å
except: pass

results = {}
for g, a in ADDRESSES.items():
    print(f"üì° –ó–±—ñ—Ä {g}...")
    results[g] = get_schedule(scraper, g, a)
    time.sleep(5) 

output = {"last_update": datetime.now().strftime("%d.%m.%Y %H:%M"), "groups": results}
with open('schedule.json', 'w', encoding='utf-8') as f:
    json.dump(output, f, ensure_ascii=False, indent=4)
